{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/ubuntu/miniconda3/lib/python3.10/site-packages (0.6.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "!pip install einops\n",
    "!pip install -q -U pandas\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install gradio\n",
    "!pip install sentencepiece\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from peft import PeftModel  \n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, trust_remote_code=True)\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# Check if the tokenizer has a pad token; if not, assign one.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate instruction and input text\n",
    "    input_text = [' '.join(t) for t in zip(examples[\"instruction\"], examples[\"context\"])]\n",
    "    output_text = examples[\"response\"]\n",
    "\n",
    "    # Tokenize inputs and outputs\n",
    "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(output_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Ensure 'labels' are -100 where we don't compute loss (this is necessary for huggingface/transformers)\n",
    "    outputs['input_ids'] = [x if x != tokenizer.pad_token_id else -100 for x in outputs['input_ids']]\n",
    "\n",
    "    return {\"input_ids\": inputs['input_ids'], \"attention_mask\": inputs['attention_mask'], \"labels\": outputs['input_ids']}\n",
    "\n",
    "dataset = data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"context\", \"response\"])\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Split the data into 80% for training and 20% for evaluation\n",
    "split_data = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Now we update the dataset with the new split data\n",
    "dataset = DatasetDict({\n",
    "    'train': split_data['train'],\n",
    "    'eval': split_data['test']\n",
    "})\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,  # Reduced batch size to manage GPU memory\n",
    "    gradient_accumulation_steps=3,\n",
    "    warmup_steps=2,\n",
    "    #num_train_epochs=3,\n",
    "    max_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    #int8=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['eval'],\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up GPU memory before starting training\n",
    "trainer.train()\n",
    "torch.cuda.empty_cache()  # Free up GPU memory after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ubuntu/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
      "100%|██████████| 1/1 [00:00<00:00, 567.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('databricks/databricks-dolly-15k')\n",
    "\n",
    "print(type(dataset))\n",
    "\n",
    "# You might have multiple splits like 'train', 'test'. Here I'm assuming you want to split the 'train' set.\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "train_test_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_datasets = train_test_split['train']\n",
    "test_datasets = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_format(type='torch', columns=['instruction', 'context', 'response', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the data into 80% for training and 20% for evaluation\n",
    "split_data = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Now we update the dataset with the new split data\n",
    "dataset = DatasetDict({\n",
    "    'train': split_data['train'],\n",
    "    'eval': split_data['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find a dataset script at /home/ubuntu/work/databricks/another-dataset-15k/another-dataset-15k.py or any data file in the same directory. Couldn't find 'databricks/another-dataset-15k' on the Hugging Face Hub either: FileNotFoundError: Dataset 'databricks/another-dataset-15k' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the new dataset, for example \"databricks/another-dataset-15k\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m another_data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mdatabricks/another-dataset-15k\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Tokenize the new dataset\u001b[39;00m\n\u001b[1;32m      5\u001b[0m another_dataset \u001b[39m=\u001b[39m another_data\u001b[39m.\u001b[39mmap(tokenize_function, batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39minstruction\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/load.py:1773\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1768\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1769\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1770\u001b[0m )\n\u001b[1;32m   1772\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1773\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1774\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1775\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1776\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1777\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1778\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1779\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1780\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1781\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1782\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1783\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1784\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1785\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1786\u001b[0m )\n\u001b[1;32m   1788\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/load.py:1502\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1501\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1502\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1503\u001b[0m     path,\n\u001b[1;32m   1504\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1505\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1506\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1507\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1508\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1509\u001b[0m )\n\u001b[1;32m   1511\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/datasets/load.py:1215\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e1, \u001b[39mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1215\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1216\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a dataset script at \u001b[39m\u001b[39m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[39m}\u001b[39;00m\u001b[39m or any data file in the same directory. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1217\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m on the Hugging Face Hub either: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e1)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me1\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1218\u001b[0m                 ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m             \u001b[39mraise\u001b[39;00m e1 \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Couldn't find a dataset script at /home/ubuntu/work/databricks/another-dataset-15k/another-dataset-15k.py or any data file in the same directory. Couldn't find 'databricks/another-dataset-15k' on the Hugging Face Hub either: FileNotFoundError: Dataset 'databricks/another-dataset-15k' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`."
     ]
    }
   ],
   "source": [
    "# Load the new dataset, for example \"databricks/another-dataset-15k\"\n",
    "another_data = load_dataset(\"databricks/another-dataset-15k\")\n",
    "\n",
    "# Tokenize the new dataset\n",
    "another_dataset = another_data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"context\", \"response\"])\n",
    "\n",
    "# Set the format for each split in the new dataset\n",
    "for split in another_dataset.keys():\n",
    "    another_dataset[split].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Use the new dataset in the trainer\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    train_dataset=another_dataset['train'],  # Use the new dataset for training\n",
    "    eval_dataset=another_dataset['eval'],  # Use the new dataset for evaluation\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached datasets:\n",
      "['_home_ubuntu_.cache_huggingface_datasets_databricks___json_databricks--databricks-dolly-15k-6e0f9ea7eaa0ee08_0.0.0_e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4.lock', 'downloads', 'databricks___json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
    "if os.path.isdir(cache_dir):\n",
    "    print(\"Cached datasets:\")\n",
    "    print(os.listdir(cache_dir))\n",
    "else:\n",
    "    print(\"No cached datasets found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/TimDettmers/bitsandbytes.git\n",
      "  Cloning https://github.com/TimDettmers/bitsandbytes.git to /tmp/pip-req-build-yulhiein\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/TimDettmers/bitsandbytes.git /tmp/pip-req-build-yulhiein\n",
      "  Resolved https://github.com/TimDettmers/bitsandbytes.git to commit ac5550a0238286377ee3f58a85aeba1c40493e17\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/TimDettmers/bitsandbytes.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BitsAndBytesConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mPYTORCH_CUDA_ALLOC_CONF\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmax_split_size_mb=500\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m model_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtiiuae/falcon-7b\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     12\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[1;32m     19\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(model_id, quantization_config\u001b[39m=\u001b[39mbnb_config, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device_map\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m0\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BitsAndBytesConfig' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=500'\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, trust_remote_code=True, device_map={\"\":0})\n",
    "     \n",
    "\n",
    "# Check if the tokenizer has a pad token; if not, assign one.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate instruction and input text\n",
    "    input_text = [' '.join(t) for t in zip(examples[\"instruction\"], examples[\"context\"])]\n",
    "    output_text = examples[\"response\"]\n",
    "\n",
    "    # Tokenize inputs and outputs\n",
    "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(output_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Ensure 'labels' are -100 where we don't compute loss (this is necessary for huggingface/transformers)\n",
    "    outputs['input_ids'] = [x if x != tokenizer.pad_token_id else -100 for x in outputs['input_ids']]\n",
    "\n",
    "    return {\"input_ids\": inputs['input_ids'], \"attention_mask\": inputs['attention_mask'], \"labels\": outputs['input_ids']}\n",
    "\n",
    "dataset = data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"context\", \"response\"])\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Split the data into 80% for training and 20% for evaluation\n",
    "split_data = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Now we update the dataset with the new split data\n",
    "dataset = DatasetDict({\n",
    "    'train': split_data['train'],\n",
    "    'eval': split_data['test']\n",
    "})\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,  # Reduced batch size to manage GPU memory\n",
    "    gradient_accumulation_steps=3,\n",
    "    warmup_steps=2,\n",
    "    #num_train_epochs=3,\n",
    "    max_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    #int8=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['eval'],\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up GPU memory before starting training\n",
    "trainer.train()\n",
    "torch.cuda.empty_cache()  # Free up GPU memory after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
