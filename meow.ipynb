{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "!pip install einops\n",
    "!pip install -q -U pandas\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install gradio\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /home/ubuntu/.conda/lib/python3.11/site-packages (0.6.1)\n",
      "Requirement already satisfied: gradio in /home/ubuntu/.conda/lib/python3.11/site-packages (3.33.1)\n",
      "Requirement already satisfied: aiofiles in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (23.1.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (3.8.4)\n",
      "Requirement already satisfied: altair>=4.2.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (5.0.1)\n",
      "Requirement already satisfied: fastapi in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.96.0)\n",
      "Requirement already satisfied: ffmpy in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: gradio-client>=0.2.4 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.2.5)\n",
      "Requirement already satisfied: httpx in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.15.1)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: markupsafe in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.3.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (1.24.3)\n",
      "Requirement already satisfied: orjson in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (3.9.0)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (9.5.0)\n",
      "Requirement already satisfied: pydantic in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (1.10.8)\n",
      "Requirement already satisfied: pydub in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: pygments>=2.12.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (2.15.1)\n",
      "Requirement already satisfied: python-multipart in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: pyyaml in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (2.31.0)\n",
      "Requirement already satisfied: semantic-version in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (4.6.3)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from altair>=4.2.0->gradio) (4.17.3)\n",
      "Requirement already satisfied: toolz in /home/ubuntu/.conda/lib/python3.11/site-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio-client>=0.2.4->gradio) (2023.5.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.conda/lib/python3.11/site-packages (from gradio-client>=0.2.4->gradio) (23.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from huggingface-hub>=0.14.0->gradio) (4.65.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (0.1.2)\n",
      "Requirement already satisfied: linkify-it-py<3,>=1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from markdown-it-py[linkify]>=2.0.0->gradio) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.conda/lib/python3.11/site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from pandas->gradio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from pandas->gradio) (2023.3)\n",
      "Requirement already satisfied: click>=7.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from uvicorn>=0.14.0->gradio) (8.1.3)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ubuntu/.conda/lib/python3.11/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.conda/lib/python3.11/site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: certifi in /home/ubuntu/.conda/lib/python3.11/site-packages (from httpx->gradio) (2023.5.7)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from httpx->gradio) (0.17.2)\n",
      "Requirement already satisfied: idna in /home/ubuntu/.conda/lib/python3.11/site-packages (from httpx->gradio) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/.conda/lib/python3.11/site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from matplotlib->gradio) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.conda/lib/python3.11/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from matplotlib->gradio) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from matplotlib->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.conda/lib/python3.11/site-packages (from requests->gradio) (2.0.2)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.7.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /home/ubuntu/.conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
      "Requirement already satisfied: uc-micro-py in /home/ubuntu/.conda/lib/python3.11/site-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->gradio) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ubuntu/.conda/lib/python3.11/site-packages (0.1.99)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpeft\u001b[39;00m \u001b[39mimport\u001b[39;00m PeftModel  \n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset, DatasetDict\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/peft/__init__.py:22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.4.0.dev0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmapping\u001b[39;00m \u001b[39mimport\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING, PEFT_TYPE_TO_CONFIG_MAPPING, get_peft_config, get_peft_model\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     PeftModel,\n\u001b[1;32m     25\u001b[0m     PeftModelForCausalLM,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     31\u001b[0m     AdaptionPromptConfig,\n\u001b[1;32m     32\u001b[0m     AdaptionPromptModel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     PromptTuningInit,\n\u001b[1;32m     45\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/peft/mapping.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# coding=utf-8\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Copyright 2023-present the HuggingFace Inc. team.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpeft_model\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     PeftModel,\n\u001b[1;32m     18\u001b[0m     PeftModelForCausalLM,\n\u001b[1;32m     19\u001b[0m     PeftModelForSeq2SeqLM,\n\u001b[1;32m     20\u001b[0m     PeftModelForSequenceClassification,\n\u001b[1;32m     21\u001b[0m     PeftModelForTokenClassification,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     24\u001b[0m     AdaLoraConfig,\n\u001b[1;32m     25\u001b[0m     AdaptionPromptConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     PromptTuningConfig,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PromptLearningConfig\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/peft/peft_model.py:31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_outputs\u001b[39;00m \u001b[39mimport\u001b[39;00m SequenceClassifierOutput, TokenClassifierOutput\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m PushToHubMixin\n\u001b[0;32m---> 31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtuners\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     AdaLoraModel,\n\u001b[1;32m     33\u001b[0m     AdaptionPromptModel,\n\u001b[1;32m     34\u001b[0m     LoraModel,\n\u001b[1;32m     35\u001b[0m     PrefixEncoder,\n\u001b[1;32m     36\u001b[0m     PromptEmbedding,\n\u001b[1;32m     37\u001b[0m     PromptEncoder,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     40\u001b[0m     TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n\u001b[1;32m     41\u001b[0m     WEIGHTS_NAME,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     shift_tokens_right,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     55\u001b[0m PEFT_TYPE_TO_MODEL_MAPPING \u001b[39m=\u001b[39m {\n\u001b[1;32m     56\u001b[0m     PeftType\u001b[39m.\u001b[39mLORA: LoraModel,\n\u001b[1;32m     57\u001b[0m     PeftType\u001b[39m.\u001b[39mPROMPT_TUNING: PromptEmbedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     PeftType\u001b[39m.\u001b[39mADAPTION_PROMPT: AdaptionPromptModel,\n\u001b[1;32m     62\u001b[0m }\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/peft/tuners/__init__.py:21\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# module, but to preserve other warnings. So, don't check this module at all\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madaption_prompt\u001b[39;00m \u001b[39mimport\u001b[39;00m AdaptionPromptConfig, AdaptionPromptModel\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlora\u001b[39;00m \u001b[39mimport\u001b[39;00m LoraConfig, LoraModel\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madalora\u001b[39;00m \u001b[39mimport\u001b[39;00m AdaLoraConfig, AdaLoraModel\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mp_tuning\u001b[39;00m \u001b[39mimport\u001b[39;00m PromptEncoder, PromptEncoderConfig, PromptEncoderReparameterizationType\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/peft/tuners/lora.py:41\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     COMMON_LAYERS_PATTERN,\n\u001b[1;32m     30\u001b[0m     TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     transpose,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLoraConfig\u001b[39;00m(PeftConfig):\n\u001b[1;32m     46\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m    This is the configuration class to store the configuration of a [`LoraModel`].\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39m            pattern is not in the common layers pattern.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[1;32m     11\u001b[0m T \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, bound\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorch.nn.Module\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/optim/__init__.py:8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madagrad\u001b[39;00m \u001b[39mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madam\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madamw\u001b[39;00m \u001b[39mimport\u001b[39;00m AdamW, AdamW8bit, AdamW32bit, PagedAdamW, PagedAdamW8bit, PagedAdamW32bit\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/optim/adagrad.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m Optimizer1State\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAdagrad\u001b[39;00m(Optimizer1State):\n\u001b[1;32m      9\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     10\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m     11\u001b[0m         params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         block_wise\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     22\u001b[0m     ):\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mimport\u001b[39;00m chain\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMockArgs\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, initial_data):\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/bitsandbytes/functional.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m \u001b[39mimport\u001b[39;00m norm\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m reduce  \u001b[39m# Required in Python 3\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from peft import PeftModel  \n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, trust_remote_code=True)\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# Check if the tokenizer has a pad token; if not, assign one.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "\n",
    "# Define the tokenization function\n",
    "def tokenize_function(examples):\n",
    "    # Concatenate instruction and input text\n",
    "    input_text = [' '.join(t) for t in zip(examples[\"instruction\"], examples[\"context\"])]\n",
    "    output_text = examples[\"response\"]\n",
    "\n",
    "    # Tokenize inputs and outputs\n",
    "    inputs = tokenizer(input_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs = tokenizer(output_text, padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Ensure 'labels' are -100 where we don't compute loss (this is necessary for huggingface/transformers)\n",
    "    outputs['input_ids'] = [x if x != tokenizer.pad_token_id else -100 for x in outputs['input_ids']]\n",
    "\n",
    "    return {\"input_ids\": inputs['input_ids'], \"attention_mask\": inputs['attention_mask'], \"labels\": outputs['input_ids']}\n",
    "\n",
    "dataset = data.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"context\", \"response\"])\n",
    "dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Split the data into 80% for training and 20% for evaluation\n",
    "split_data = dataset['train'].train_test_split(test_size=0.2)\n",
    "\n",
    "# Now we update the dataset with the new split data\n",
    "dataset = DatasetDict({\n",
    "    'train': split_data['train'],\n",
    "    'eval': split_data['test']\n",
    "})\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,  # Reduced batch size to manage GPU memory\n",
    "    gradient_accumulation_steps=3,\n",
    "    warmup_steps=2,\n",
    "    #num_train_epochs=3,\n",
    "    max_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    #int8=True,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    optim=\"paged_adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['eval'],\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "torch.cuda.empty_cache()  # Free up GPU memory before starting training\n",
    "trainer.train()\n",
    "torch.cuda.empty_cache()  # Free up GPU memory after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
